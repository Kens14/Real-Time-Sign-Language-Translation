{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import mediapipe as mp\n",
        "import tensorflow\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {},
      "id": "2988604e"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mp_holistic =  mp.solutions.holistic # Holistic model\n",
        "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mediapipe_detection(image, model):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
        "    image.flags.writeable = False                  # Image is no longer writeable\n",
        "    results = model.process(image)                 # Make prediction\n",
        "    image.flags.writeable = True                   # Image is now writeable \n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
        "    return image, results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def draw_landmarks(image, results):\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def draw_styled_landmarks(image, results):\n",
        "    # Draw face connections\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
        "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
        "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
        "                             ) \n",
        "    # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
        "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
        "                             ) \n",
        "    # Draw left hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
        "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
        "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
        "                             ) \n",
        "    # Draw right hand connections  \n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
        "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
        "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
        "                             ) \n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {},
      "id": "ebf01057"
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(0)\n",
        "# Set mediapipe model \n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "    while cap.isOpened():\n",
        "\n",
        "        # Read feed\n",
        "        \n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Make detections\n",
        "        image, results = mediapipe_detection(frame, holistic)\n",
        "        print(results)\n",
        "        \n",
        "        # Draw landmarks\n",
        "        draw_styled_landmarks(image, results)\n",
        "        cv2.putText(image, 'Raise your left hand for intialisation & press q',(60,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "        #resizing the cv feed\n",
        "        cv2.namedWindow(\"intialize\", cv2.WINDOW_NORMAL)\n",
        "        cv2.resizeWindow(\"intialize\", 1280, 720)\n",
        "        \n",
        "        # Show to screen   \n",
        "        cv2.imshow('intialize', image)\n",
        "\n",
        "        # Break gracefully\n",
        "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "            break\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {},
      "id": "6da46db6"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "e876557d"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "51ad5170"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_keypoints(results):\n",
        "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
        "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "    return np.concatenate([pose, face, lh, rh])\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {},
      "id": "75efdcb5"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DATA_PATH = os.path.join('../MP_Data/Phrases') \n",
        "\n",
        " # Thirty videos worth of data\n",
        "no_sequences = 30\n",
        " # Videos are going to be 30 frames in length\n",
        "sequence_length = 30\n",
        "actions_new = np.array([])"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {},
      "id": "51615e67"
    },
    {
      "cell_type": "code",
      "source": [
        "def create(model_to_save):\n",
        "    # Path for exported data, numpy arrays\n",
        "    model_new = model_to_save\n",
        "\n",
        "    # Actions that we try to detect\n",
        "    #demo data1'Emergency','Hello','Fine','wait'\n",
        "    #demo data2 'deaf','finish','help','good morning'\n",
        "\n",
        "\n",
        "    x = 0\n",
        "    n = int(input(\"Enter number of actions < 8 : \"))\n",
        "    x = x+1\n",
        "    if n==0:\n",
        "        print(\"Enter a number greater than 0 \")\n",
        "        n = int(input(\"Enter number of actions < 8 : \"))\n",
        "    else:    \n",
        "        for i in range(0, n):\n",
        "            ele = (input(\"Name of the signs: \"))\n",
        "            #appending to the names file\n",
        "            # Filename to write\n",
        "            filename =(\"Names/\"+ model_new  + \".names\")\n",
        "            \n",
        "            global actions_new \n",
        "            actions_new = np.append(actions_new,ele)\n",
        "            \n",
        "            # Open the file with writing permission\n",
        "            myfile = open(filename, 'a')\n",
        "\n",
        "            # Write a line to the file\n",
        "            if x>1 or i>0:\n",
        "                myfile.write(\"\\n\"+ele)\n",
        "            else:\n",
        "                myfile.write(ele)\n",
        "            # Close the file\n",
        "            myfile.close()\n",
        "        #crete path\n",
        "        for action in actions_new: \n",
        "            for sequence in range(no_sequences):\n",
        "                try: \n",
        "                    os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    # Set mediapipe model \n",
        "    with mp_holistic.Holistic( min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "\n",
        "        # NEW LOOP\n",
        "        for action in actions_new:\n",
        "        # Loop through actions\n",
        "            # Loop through sequences aka videos\n",
        "            for sequence in range(no_sequences):\n",
        "                # Loop through video length aka sequence length\n",
        "                for frame_num in range(sequence_length):\n",
        "\n",
        "                    # Read feed\n",
        "                    ret, frame = cap.read()\n",
        "\n",
        "                    # Make detections\n",
        "                    image, results = mediapipe_detection(frame, holistic)\n",
        "                    print(results)\n",
        "\n",
        "                    # Draw landmarks\n",
        "                    draw_styled_landmarks(image, results)\n",
        "\n",
        "                    # NEW Apply wait logic\n",
        "                    if frame_num == 0: \n",
        "                        cv2.putText(image, 'STARTING COLLECTION', (120,200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
        "                        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (60,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "                        #resizing the cv feed\n",
        "                        cv2.namedWindow(\"OpenCV Feed\", cv2.WINDOW_NORMAL)\n",
        "                        cv2.resizeWindow(\"OpenCV Feed\", 1280, 720)\n",
        "                        # Show to screen\n",
        "                        cv2.imshow('OpenCV Feed', image)\n",
        "                        cv2.waitKey(3000)\n",
        "                        \n",
        "                    else: \n",
        "                        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence),(60,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
        "                      \n",
        "                    # NEW Export keypoints\n",
        "                    keypoints = extract_keypoints(results)\n",
        "                    npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
        "                    np.save(npy_path, keypoints)\n",
        "\n",
        "                    # Break gracefully\n",
        "                    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "                        break\n",
        "\n",
        "        \n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "        \n",
        "    trainm(model_new)\n",
        "\n",
        "    \n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {},
      "id": "ac679675"
    },
    {
      "cell_type": "code",
      "source": [
        "def trainm(model_new):    \n",
        "    model_to_save = model_new \n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "    label_map = {label:num for num, label in enumerate(actions_new)}\n",
        "\n",
        "\n",
        "    sequences= []\n",
        "    labels = []\n",
        "    for action in actions_new:\n",
        "        for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
        "            window = []\n",
        "            for frame_num in range(sequence_length):\n",
        "                res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
        "                window.append(res)\n",
        "            sequences.append(window)\n",
        "            labels.append(label_map[action])\n",
        "\n",
        "\n",
        "    np.array(sequences).shape\n",
        "\n",
        "    np.array(labels).shape\n",
        "\n",
        "    X = np.array(sequences)\n",
        "\n",
        "\n",
        "    X.shape\n",
        "\n",
        "\n",
        "    y = to_categorical(labels).astype(int)\n",
        "\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
        "\n",
        "    y_test.shape\n",
        "    # \n",
        "    # ## 7. Build and Train LSTM Neural Network\n",
        "\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import LSTM, Dense\n",
        "    from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "    log_dir = os.path.join('Logs')\n",
        "    tb_callback = TensorBoard(log_dir=log_dir)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(30,1662)))\n",
        "    model.add(LSTM(128, return_sequences=True, activation='tanh'))\n",
        "    model.add(LSTM(64, return_sequences=False, activation='tanh'))\n",
        "    model.add(Dense(64, activation='tanh'))\n",
        "    model.add(Dense(32, activation='tanh'))\n",
        "    model.add(Dense(actions_new.shape[0], activation='softmax')) \n",
        "\n",
        "    ACCURACY_THRESHOLD = 0.96\n",
        "    LOSS_THRESHOLD= 0.16\n",
        "    # Implement callback function to stop training\n",
        "    # when accuracy reaches e.g. ACCURACY_THRESHOLD = 0.95\n",
        "    class myCallback(tensorflow.keras.callbacks.Callback): \n",
        "        def on_epoch_end(self, epoch, logs={}): \n",
        "            if (logs.get('categorical_accuracy') > ACCURACY_THRESHOLD) and (logs.get('loss') < LOSS_THRESHOLD):  \n",
        "                print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(ACCURACY_THRESHOLD*100))   \n",
        "                self.model.stop_training = True\n",
        "\n",
        "          \n",
        "\n",
        "\n",
        "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'], callbacks = myCallback())\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=100)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # ## 8. Make Predictions\n",
        "\n",
        "\n",
        "    res = model.predict(X_test)\n",
        "\n",
        "    actions_new[np.argmax(res[0])]\n",
        "\n",
        "    actions_new[np.argmax(y_test[0])]\n",
        "\n",
        "    # ## 9. Save Weights\n",
        "\n",
        "    model.save('../models/'+ model_to_save +'.h5')\n",
        "    \n",
        "    answer = input(\"Do you want to add more signs? (Y/N): \") \n",
        "    if answer == \"Y\" or answer == \"y\": \n",
        "        create()      \n",
        "    elif  answer == \"N\" or answer == \"n\": \n",
        "        print(\"Your Model Will Now Be Trained Please Wait.\")\n",
        "        pass\n",
        "\n",
        "    else:\n",
        "        print(\"Please Choose Y(yes)/N(no).\")\n"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {},
      "id": "42a7458c"
    },
    {
      "cell_type": "code",
      "source": [
        "def switch():\n",
        "# This will guide the user to choose option\n",
        "    print(\"1: Create your own model\\n2: Exiting models\\n3: delete\\n4: Exit\")\n",
        " \n",
        "# This will take option from user    \n",
        "    option = int(input(\" your option : \"))\n",
        "# Create your own sign\n",
        "    def create_model():\n",
        "        model_to_save=input(\"What would you like to name the model? \")\n",
        "        create(model_to_save)\n",
        "        switch()\n",
        "# use existing signs\n",
        "    def existing():\n",
        "        exec(open('preditc.py').read())\n",
        "        execfile()\n",
        "        switch()\n",
        "    def delete():\n",
        "        exec(open('delete.py').read())\n",
        "        switch()\n",
        "        \n",
        "    def Exit():\n",
        "        exit()\n",
        "# If user enters invalid option then this method will be called \n",
        "    def default():\n",
        "        print(\"Incorrect option\")\n",
        "        switch()\n",
        "        \n",
        "# Dictionary Mapping\n",
        "    dict = {\n",
        "        1 : create_model,\n",
        "        2 : existing,\n",
        "        3 : delete,\n",
        "        4 : Exit,      \n",
        "    }\n",
        "    dict.get(option,default)() # get() method returns the function matching the argument\n",
        "\n",
        "switch() # Call switch() method"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "09e76567"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "82796367"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "0b6edb2b"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "9993195d"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "919c0233"
    },
    {
      "cell_type": "raw",
      "source": [],
      "metadata": {},
      "id": "b9348d3a"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "27cd9b6d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}